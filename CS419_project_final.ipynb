{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS419_project_final",
      "provenance": [],
      "collapsed_sections": [
        "hY3h6EKWFi0D",
        "OaOB3UauZ_Ue",
        "jpTVOVXNaHkG",
        "7CNTAA28FVx4",
        "xCUxeTsOveiV",
        "3r8mErfwG3Mq",
        "sedvUBoyqQUZ",
        "5b5stTBrW01r",
        "nuUhZU2vLZoD",
        "oce7ZdP9goWm",
        "O-ONUdulY1r8"
      ],
      "authorship_tag": "ABX9TyN4XE/mGL71zPxcxTmxyMFz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Team-Tensor/Face-Recognition/blob/main/CS419_project_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwhefNh0uGdO"
      },
      "source": [
        "# **CS419 Project : Team Tensor**\n",
        "## **Comparison of Face Recognition models using Neural Networks and Principal Component Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY3h6EKWFi0D"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hmqYWxmDyDY"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from numpy import matlib\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch as torch\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "from time import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.preprocessing import Normalizer, LabelEncoder\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "!pip install pytorchtools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEcY5bShFpGW"
      },
      "source": [
        "# Loading Raw Image Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaOB3UauZ_Ue"
      },
      "source": [
        "### Extracting images to arrays and saving to **CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp7ka-eCVlUK"
      },
      "source": [
        "# for dirname, _, filenames in os.walk('/content/gdrive/MyDrive/yalefaces_extended'):\n",
        "#     filecount=len(filenames)\n",
        "#     y=torch.empty(filecount, device='cuda:0')\n",
        "#     X=torch.empty(filecount, 25600, device='cuda:0')    #160x160=25600\n",
        "#     i=0\n",
        "#     with tqdm(total=filecount) as pbar:\n",
        "#       for filename in filenames:\n",
        "#             b = cv2.imread(os.path.join(dirname, filename),0)\n",
        "#             b = cv2.resize(b, (160,160),interpolation = cv2.INTER_AREA)\n",
        "\n",
        "#             X[i]=torch.from_numpy(b.ravel())\n",
        "#             y[i]=int(filename[5:7])\n",
        "\n",
        "#             pbar.update(1)\n",
        "#             i=i+1\n",
        "#             if i==filecount:\n",
        "#               break\n",
        "\n",
        "# np.savetxt('/content/gdrive/MyDrive/yalefaces_extended.csv', \n",
        "#            np.concatenate((X.cpu(),y.cpu()[:,None]),axis=1),\n",
        "#            delimiter =\", \", \n",
        "#            fmt ='%d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpTVOVXNaHkG"
      },
      "source": [
        "### Using pre-extracted images from CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-CegiR_EWlf"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysYT0mx3Zkve"
      },
      "source": [
        "data=np.loadtxt('/content/gdrive/MyDrive/yalefaces_extended.csv', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov6R8R_Z2S7z"
      },
      "source": [
        "X=np.array(data[:,:-1], dtype=np.uint8)\n",
        "y=data[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AXhra7khMhQ"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akulUwyhFJXL"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CNTAA28FVx4"
      },
      "source": [
        "## Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBlG7idcEyUA"
      },
      "source": [
        "pca = PCA(n_components=1000).fit(x_train)\n",
        "# # Plot cumulative variance captured by the principal components\n",
        "# plt.plot(np.cumsum(pca.explained_variance_ratio_))  \n",
        "\n",
        "fig, axes = plt.subplots(3, 8, figsize=(9, 4), subplot_kw={'xticks':[], 'yticks':[]})\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(pca.components_[i].reshape(160, 160), cmap='gray')\n",
        "    ax.set_title(\"PC \" + str(i+1))\n",
        "\n",
        "Xtrain_pca = normalize(pca.transform(x_train), axis=0)\n",
        "Xtest_pca = normalize(pca.transform(x_test), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCUxeTsOveiV"
      },
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYwwa4pDYykL"
      },
      "source": [
        "### Generate features using VGG16 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlS_w54XvgyS"
      },
      "source": [
        "# n=2470\n",
        "# X_feat = np.empty([n, 224, 224, 3])\n",
        "# lb = np.empty(n)\n",
        "# model = VGG16(weights='imagenet', include_top=False)\n",
        "# for i in tqdm(range(n)):\n",
        "#   j=np.random.randint(2470)\n",
        "#   lb[i]=y[j]\n",
        "#   img = X[j].reshape(160,160)\n",
        "#   img = cv2.resize(img, (224,224),interpolation = cv2.INTER_AREA)\n",
        "#   img = preprocess_input(img)\n",
        "\n",
        "#   b_unroll = img.ravel()\n",
        "#   b_color = np.matlib.repmat(b_unroll, 1, 3)\n",
        "#   b_color = b_color.reshape(224, 224, 3)\n",
        "\n",
        "#   X_feat[i]=b_color\n",
        "\n",
        "\n",
        "# features=model.predict(X_feat)\n",
        "\n",
        "# f2=np.empty([n, 25088])\n",
        "# for i in tqdm(range(n)):\n",
        "#   f2[i]=features[i].ravel()\n",
        "\n",
        "# # print(f2.shape)\n",
        "\n",
        "# np.savetxt('/content/gdrive/MyDrive/yalefaces_extended_vgg16-features2.csv', \n",
        "#            np.concatenate((f2,lb[:,None]),axis=1),\n",
        "#            delimiter =\", \", \n",
        "#            fmt ='%f')\n",
        "# del X_feat\n",
        "# del b_unroll\n",
        "# del b_color\n",
        "# del f2\n",
        "# del lb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OXgrM0FY8o8"
      },
      "source": [
        "### Load previously generated features using VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifj_M6ISXN2g"
      },
      "source": [
        "data=np.loadtxt('/content/gdrive/MyDrive/yalefaces_extended_vgg16-features2.csv', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2j1r6eLXe-O"
      },
      "source": [
        "X_vgg=data[:,:-1]\n",
        "y_vgg=data[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gKfEXXc-Dxr"
      },
      "source": [
        "Xtrain_vgg, Xtest_vgg, y_train_vgg, y_test_vgg = train_test_split(X_vgg, y_vgg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXBWWhumGzLG"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r8mErfwG3Mq"
      },
      "source": [
        "## SVM (with PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFdExGckW4iJ"
      },
      "source": [
        "clf = SVC(kernel='rbf',C=1e3,gamma=1e-2)\n",
        "start_time = time()\n",
        "clf = clf.fit(Xtrain_pca, y_train)\n",
        "print(\"Training Time: \", time()-start_time, \"s\")\n",
        "y_pred_svm = clf.predict(Xtest_pca)\n",
        "print(classification_report(y_test, y_pred_svm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sedvUBoyqQUZ"
      },
      "source": [
        "### Improve performance by hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56a7vwMKlup_"
      },
      "source": [
        "scorer = make_scorer(f1_score, average='macro')\n",
        "\n",
        "tuned_parameters = [{'kernel': ['rbf','poly','linear','sigmoid'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
        "                     'C': [10, 100, 1000, 10000]}]\n",
        "scores = ['precision', 'recall']\n",
        "#for score in scores:\n",
        "\n",
        "clf = GridSearchCV(SVC(), tuned_parameters, scoring=scorer)\n",
        "clf.fit(Normalizer().fit(Xtrain_pca).transform(Xtrain_pca), y_train)\n",
        "params=clf.best_params_\n",
        "print(clf.best_params_)\n",
        "cv= SVC(C=params[\"C\"], gamma= params[\"gamma\"], kernel=params[\"kernel\"])\n",
        "cv.fit(Normalizer().fit(Xtrain_pca).transform(Xtrain_pca), y_train)\n",
        "\n",
        "y_pred = cv.predict(Normalizer().fit(Xtest_pca).transform(Xtest_pca))\n",
        "pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b5stTBrW01r"
      },
      "source": [
        "## SVM (with VGG16)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPHyLMLREzuJ"
      },
      "source": [
        "clf = SVC(kernel='rbf',C=100,gamma=1e-2)\n",
        "start_time = time()\n",
        "clf = clf.fit(Xtrain_vgg, y_train_vgg)\n",
        "print(\"Training Time: \", time()-start_time, \"s\")\n",
        "\n",
        "y_pred_svm_vgg = clf.predict(Xtest_vgg)\n",
        "print(classification_report(y_test_vgg, y_pred_svm_vgg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuUhZU2vLZoD"
      },
      "source": [
        "## Feed-forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux_08qCgWtYb"
      },
      "source": [
        "class net(nn.Module):\n",
        "  def __init__(self,input_dim):\n",
        "    super(net,self).__init__()\n",
        "    self.bn1=nn.BatchNorm1d(input_dim)\n",
        "    self.fc1=nn.Linear(input_dim,128)\n",
        "    self.g1=nn.ReLU()\n",
        "    #self.bn2=nn.BatchNorm1d(128)\n",
        "    #self.d1=nn.Dropout(p=0.4)\n",
        "    self.fc2=nn.Linear(128,128)\n",
        "    self.g2=nn.ReLU()\n",
        "    #self.bn3=nn.BatchNorm1d(256)\n",
        "    self.fc3=nn.Linear(128,128)\n",
        "    self.g3=nn.ReLU()\n",
        "    self.fc4=nn.Linear(128,39)\n",
        "    #self.g4=F.softmax(out, dim=1)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out=self.bn1(x)\n",
        "    out=x\n",
        "    out=self.fc1(out)\n",
        "    out=self.g1(out)\n",
        "    #out=F.dropout(out,p=0.2)\n",
        "    #out=self.bn2(out)\n",
        "    #out=self.d1(out)\n",
        "    out=self.fc2(out)\n",
        "    out=self.g2(out)\n",
        "    #out=F.dropout(out,p=0.2)\n",
        "    #out=self.bn3(out)\n",
        "    out=self.fc3(out)\n",
        "    out=self.g3(out)\n",
        "    out=self.fc4(out)\n",
        "    #out=self.g4(out)\n",
        "    return out\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.SGD(my_net.parameters(), lr=0.003)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-MW36B6W-qd"
      },
      "source": [
        "def ffnn_output(x_train,y_train,x_test,y_test,n_iters=1000):\n",
        "  x_train=x_train.astype(np.float32)\n",
        "  x_test=x_test.astype(np.float32)\n",
        "  input=torch.tensor(x_train,device=device)\n",
        "  test_input=torch.tensor(x_test,device=device)\n",
        "  labels=torch.tensor(y_train-1,dtype=torch.long,device=device)\n",
        "  test_labels=torch.tensor(y_test-1,dtype=torch.long,device=device)\n",
        "\n",
        "  loss_record=[]\n",
        "  my_net=net(x_train.shape[1])\n",
        "  my_net.to(device)\n",
        "  m=x_train.shape[0]\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(my_net.parameters(), lr=0.003)\n",
        "  start_time=time()\n",
        "  for i in tqdm(range(n_iters)):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = my_net(input)\n",
        "\n",
        "    \n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    \n",
        "    loss.backward()\n",
        "\n",
        "    \n",
        "    optimizer.step()\n",
        "\n",
        "    loss_record.append(loss.item()/m)\n",
        "\n",
        "    \n",
        "  print(\"Training Time: \", time()-start_time, \"s\")\n",
        "  plt.plot(loss_record)\n",
        "\n",
        "  y_pred=my_net(test_input)\n",
        "  y_pred=y_pred.cpu().detach().numpy()\n",
        "  \n",
        "  print(classification_report(test_labels.cpu(), np.argmax(y_pred,axis=1)))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXCBsq_HaPPq"
      },
      "source": [
        "pca_X_csv=X\n",
        "pca = PCA(n_components=1000).fit_transform(pca_X_csv)\n",
        "pca_x_train, pca_x_test, pca_y_train, pca_y_test = train_test_split(pca, y)\n",
        "ffnn_output(pca_x_train,pca_y_train, pca_x_test, pca_y_test,1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oce7ZdP9goWm"
      },
      "source": [
        "## Logistic Regression (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhygicMybHDd"
      },
      "source": [
        "x_train=normalize(Xtrain_pca, axis=0)\n",
        "x_test=normalize(Xtest_pca, axis=0)\n",
        "\n",
        "# y_train=pca_y_train \n",
        "# y_test=pca_y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld_YsYiFdsbZ"
      },
      "source": [
        "def log_reg_output(x_train,y_train,x_test,y_test):\n",
        "  log_reg=LogisticRegression(max_iter=1000)\n",
        "\n",
        "  start_time=time()\n",
        "  log_reg.fit(x_train,y_train)\n",
        "  print(\"Training Time: \", time()-start_time, \"s\")\n",
        "\n",
        "  y_pred_logreg=log_reg.predict(x_test)\n",
        "\n",
        "  print(classification_report(y_test,y_pred_logreg))\n",
        "\n",
        "log_reg_output(x_train,y_train,x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-ONUdulY1r8"
      },
      "source": [
        "## Logistic Regression (VGG16)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKW98xkwY1r9"
      },
      "source": [
        "x_train=normalize(Xtrain_vgg, axis=0)\n",
        "x_test=normalize(Xtest_vgg, axis=0)\n",
        "\n",
        "y_train=y_train_vgg \n",
        "y_test=y_test_vgg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDTGr5FQY1r9"
      },
      "source": [
        "def log_reg_output(x_train,y_train,x_test,y_test):\n",
        "  log_reg=LogisticRegression(max_iter=1000)\n",
        "\n",
        "  start_time=time()\n",
        "  log_reg.fit(x_train,y_train)\n",
        "  print(\"Training Time: \", time()-start_time, \"s\")\n",
        "\n",
        "  y_pred_logreg=log_reg.predict(x_test)\n",
        "\n",
        "  print(classification_report(y_test,y_pred_logreg))\n",
        "\n",
        "log_reg_output(x_train,y_train,x_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RZNjfI0vsBp"
      },
      "source": [
        "## Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe57osR2waxf"
      },
      "source": [
        "x_train_cnn=np.resize(x_train,(x_train.shape[0],160,160))\n",
        "x_test_cnn=np.resize(x_test,(x_test.shape[0],160,160))\n",
        "y_train_cnn=y_train\n",
        "y_test_cnn=y_test\n",
        "\n",
        "print(x_train_cnn.shape)\n",
        "print(x_test_cnn.shape)\n",
        "print(y_train_cnn.shape)\n",
        "print(y_test_cnn.shape)\n",
        "\n",
        "x_train_cnn = x_train_cnn.reshape(x_train.shape[0], 1, 160, 160)\n",
        "x_train_cnn=x_train_cnn.astype(np.float32)\n",
        "x_train_cnn  = torch.from_numpy(x_train_cnn)\n",
        "\n",
        "# converting the target into torch format\n",
        "y_train_cnn = y_train-1\n",
        "y_train_cnn = y_train_cnn.astype(int);\n",
        "y_train_cnn = torch.from_numpy(y_train_cnn)\n",
        "\n",
        "# shape of training data\n",
        "x_train_cnn.shape, y_train_cnn.shape\n",
        "\n",
        "x_test_cnn = x_test_cnn.reshape(x_test.shape[0], 1, 160, 160)\n",
        "x_test_cnn=x_test_cnn.astype(np.float32)\n",
        "x_test_cnn  = torch.from_numpy(x_test_cnn)\n",
        "\n",
        "# converting the target into torch format\n",
        "y_test_cnn = y_test-1\n",
        "y_test_cnn = y_test_cnn.astype(int);\n",
        "y_test_cnn = torch.from_numpy(y_test_cnn)\n",
        "\n",
        "# shape of test data\n",
        "x_test_cnn.shape, y_test_cnn.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q3DbHR2xVF-"
      },
      "source": [
        "class CNN_Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN_Net, self).__init__()\n",
        "\n",
        "    self.conv1=nn.Conv2d(1,12,kernel_size=5)\n",
        "    self.a1=nn.ReLU()\n",
        "    self.pool1=nn.MaxPool2d(2,stride=2)\n",
        "    self.conv2=nn.Conv2d(12,24,kernel_size=4,stride=2)\n",
        "    self.a2=nn.ReLU()\n",
        "    self.pool2=nn.MaxPool2d(2)\n",
        "    self.conv3=nn.Conv2d(24,32,kernel_size=3,stride=2)\n",
        "    self.a3=nn.ReLU()\n",
        "    self.pool3=nn.MaxPool2d(2)\n",
        "    self.fc1 = nn.Linear(512, 120) \n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 39)\n",
        "\n",
        "  def forward(self,out):\n",
        "    out=self.conv1(out)\n",
        "    out=self.a1(out)\n",
        "    out=self.pool1(out)\n",
        "    out=self.conv2(out)\n",
        "    out=self.a2(out)\n",
        "    out=self.pool2(out)\n",
        "    out=self.conv3(out)\n",
        "    out=self.a3(out)\n",
        "    out=self.pool3(out)\n",
        "    out = torch.flatten(out, 1)\n",
        "    out=self.fc1(out)\n",
        "    out=self.fc2(out)\n",
        "    out=self.fc3(out)\n",
        "\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyHe_dZy8uZ5"
      },
      "source": [
        "model = CNN_Net()\n",
        "# defining the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# defining the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "    \n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2aTRbo49KkV"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "x_train_new, y_train_new = Variable(x_train_cnn), Variable(y_train_cnn)\n",
        "x_val_new, y_val_new = Variable(x_test_cnn[:100]), Variable(y_test_cnn[:100])\n",
        "\n",
        "\"\"\"import pytorchtools\n",
        "from pytorchtools import EarlyStopping\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
        "model.set_callbacks(callbacks)\"\"\"\n",
        "\n",
        "best_loss=100000\n",
        "flag=0\n",
        "\n",
        "def train(epoch,x_train=x_train_new, y_train=y_train_new,x_val=x_val_new, y_val=y_val_new,best_loss=best_loss):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    # getting the training set\n",
        "    #x_train, y_train = Variable(x_train_cnn), Variable(y_train_cnn)\n",
        "    # getting the validation set\n",
        "    #x_val, y_val = Variable(x_test_cnn), Variable(y_test_cnn)\n",
        "    # converting the data into GPU format\n",
        "    if torch.cuda.is_available():\n",
        "        x_train = x_train.cuda()\n",
        "        y_train = y_train.cuda()\n",
        "        x_val = x_val.cuda()\n",
        "        y_val = y_val.cuda()\n",
        "\n",
        "    # clearing the Gradients of the model parameters\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # prediction for training and validation set\n",
        "    output_train = model(x_train)\n",
        "    output_val = model(x_val)\n",
        "\n",
        "    # computing the training and validation loss\n",
        "    loss_train = criterion(output_train, y_train)\n",
        "    loss_val = criterion(output_val, y_val)\n",
        "    train_losses.append(loss_train.item()/y_train.shape[0])\n",
        "    val_losses.append(loss_val.item()/y_val.shape[0])\n",
        "\n",
        "    print('Epoch : ',epoch+1, '\\t', 'train_loss :', loss_train.item(),'\\t', 'test_loss :', loss_val.item())\n",
        "\n",
        "    # computing the updated weights of all the model parameters\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    tr_loss = loss_train.item()\n",
        "\n",
        "\n",
        "    #Early stopping\n",
        "    if best_loss > loss_val.item():\n",
        "      best_loss= loss_val.item()\n",
        "      flag = 0\n",
        "      #torch.save(model.state_dict(), \"model_\" + str(fold) + 'weight.pt')\n",
        "      torch.save(model, \"/content/best_model.pt\")\n",
        "    else:\n",
        "        flag += 1\n",
        "        print(\"Counter {} of 5\".format(es))\n",
        "\n",
        "        if flag > 4:\n",
        "          print(\"Early stopping with best_loss: \", best_loss, \"and val_loss for this epoch: \", loss_val.item(), \"...\")\n",
        "          return\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "    #if epoch%2 == 0:\n",
        "        # printing the validation loss\n",
        "        #print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTgcDafO-LXh"
      },
      "source": [
        "n_epochs = 100\n",
        "# empty list to store training losses\n",
        "train_losses = []\n",
        "# empty list to store validation losses\n",
        "val_losses = []\n",
        "# training the model\n",
        "for epoch in range(n_epochs):\n",
        "    train(epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB-B9fR6-Mc8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(val_losses, label='Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vb4t1Uceo2A"
      },
      "source": [
        "best_model=torch.load(\"/content/best_model.pt\")\n",
        "best_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaBkxzgeFtBN"
      },
      "source": [
        "x_test_new=Variable(x_test_cnn)\n",
        "y_pred_cnn=best_model(x_test_new.cuda())\n",
        "y_pred_cnn=y_pred_cnn.cpu().detach().numpy()\n",
        "   \n",
        "print(classification_report(y_test_cnn, np.argmax(y_pred_cnn,axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGeDg_FyGJhl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}